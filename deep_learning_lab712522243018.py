# -*- coding: utf-8 -*-
"""Deep Learning lab712522243018.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vaSudjiAQtcKQYQ28F4M9B9Yk9k2p5Cx

# **ex-1**
"""

model.add(Dense(1, activation='sigmoid'))
# Compile the model with binary crossentropy loss and Adam optimizer
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
# Train the model for 500/1000 epochs
model.fit(x, y, epochs=1000, verbose=0)
# Evaluate the model accuracy
loss, accuracy = model.evaluate(x, y, verbose=0) print(f'Accuracy: {accuracy* 100:.2f3%')
# Make predictions on the XOR inputs
predictions = model.predict(X)
# Print the rounded predictions (rounded to 0 or 1)
print("Predicted XOR outputs:")
print(np.round(predictions)) # Round predictions to the nearest integer (0 or 1)





!pip install tensorflow

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) # Inputs
y = np.array([0, 1, 1, 0])

model = Sequential()

model.add(Dense(2, input_dim=2, activation='relu'))
model.add(Dense (2, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.fit(x, y, epochs=1000, verbose=0)

loss, accuracy = model.evaluate(x, y, verbose=0)
print(f'Accuracy: {accuracy * 100:.2f}%')  # Changed the format specifier to .2f%
predictions = model.predict(x)
print("Predicted XOR outputs:")
print(np.round(predictions))

"""# EXP-2"""

# Install the necessary packages
!pip install tensorflow numpy

# Import necessary libraries
import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding
from tensorflow.keras.models import Model
import numpy as np

# Example English and French sentences
input_texts = ['Hello.', 'How are you?', 'Goodbye.']
target_texts = ['Bonjour.', 'Comment ça va?', 'Au revoir.']

# Vocabulary for simplicity
input_vocab = {'<pad>': 0, 'hello': 1, 'how': 2, 'are': 3, 'you': 4, 'goodbye': 5}
target_vocab = {'<pad>': 0, 'bonjour': 1, 'comment': 2, 'ça': 3, 'va': 4, 'au': 5, 'revoir': 6}
input_reverse_vocab = {v: k for k, v in input_vocab.items()}
target_reverse_vocab = {v: k for k, v in target_vocab.items()}

# Convert texts to sequences
def text_to_sequence(texts, vocab):
    return [[vocab.get(word, 0) for word in text.lower().split()] for text in texts]

encoder_input_sequences = text_to_sequence(input_texts, input_vocab)
decoder_input_sequences = text_to_sequence(target_texts, target_vocab)
decoder_target_sequences = [seq[1:] + [0] for seq in decoder_input_sequences]  # shifted right for teacher forcing

# Padding sequences
def pad_sequences(sequences, max_length):
    return tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, padding='post')

max_encoder_seq_length = max(len(seq) for seq in encoder_input_sequences)
max_decoder_seq_length = max(len(seq) for seq in decoder_input_sequences)

encoder_input_sequences = pad_sequences(encoder_input_sequences, max_encoder_seq_length)
decoder_input_sequences = pad_sequences(decoder_input_sequences, max_decoder_seq_length)
decoder_target_sequences = pad_sequences(decoder_target_sequences, max_decoder_seq_length)

# Parameters
embedding_dim = 8
hidden_units = 16
input_vocab_size = len(input_vocab)
target_vocab_size = len(target_vocab)

# Encoder
encoder_inputs = Input(shape=(None,))
encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)
encoder_lstm = LSTM(hidden_units, return_sequences=False, return_state=True)
encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding)
encoder_states = [encoder_state_h, encoder_state_c]

# Decoder
decoder_inputs = Input(shape=(None,))
decoder_embedding = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)
decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
decoder_dense = Dense(target_vocab_size, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Training
decoder_target_sequences = np.expand_dims(decoder_target_sequences, -1)
model.fit([encoder_input_sequences, decoder_input_sequences], decoder_target_sequences, epochs=10, batch_size=2)

# Inference Encoder Model
encoder_model = Model(encoder_inputs, encoder_states)

# Inference Decoder Model
decoder_state_input_h = Input(shape=(hidden_units,))
decoder_state_input_c = Input(shape=(hidden_units,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
decoder_embedding_inf = Embedding(target_vocab_size, embedding_dim)
decoder_lstm_inf = LSTM(hidden_units, return_sequences=True, return_state=True)
decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm_inf(
    decoder_embedding_inf(decoder_inputs), initial_state=decoder_states_inputs
)
decoder_states = [state_h_inf, state_c_inf]
decoder_outputs_inf = decoder_dense(decoder_outputs_inf)
decoder_model = Model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs_inf] + decoder_states
)

def decode_sequence(input_seq):
    # Encode the input sequence
    states_value = encoder_model.predict(input_seq)

    # Generate empty target sequence with only the start character
    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = target_vocab['<pad>']

    # Sampling loop
    stop_condition = False
    decoded_sentence = ''
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_token = target_reverse_vocab.get(sampled_token_index, '')

        if sampled_token == '<pad>' or len(decoded_sentence) > max_decoder_seq_length:
            stop_condition = True
        else:
            decoded_sentence += ' ' + sampled_token

        # Update the target sequence
        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_token_index
        states_value = [h, c]

    return decoded_sentence.strip()

# Example translation
input_sentence = "how are you?"
input_seq = pad_sequences([text_to_sequence([input_sentence], input_vocab)[0]], max_encoder_seq_length)
translated_sentence = decode_sequence(input_seq)
print(f'Translated Sentence: {translated_sentence}')

"""# EXP 6"""

!pip install tensorflow numpy

"""# **EXp-5**"""

# Install the necessary packages
!pip install tensorflow numpy

# Import necessary libraries
import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding
from tensorflow.keras.models import Model
import numpy as np

# Example English and French sentences
input_texts = ['Hello.', 'How are you?', 'Goodbye.']
target_texts = ['Bonjour.', 'Comment ça va?', 'Au revoir.']

# Vocabulary for simplicity
input_vocab = {'<pad>': 0, 'hello': 1, 'how': 2, 'are': 3, 'you': 4, 'goodbye': 5}
target_vocab = {'<pad>': 0, 'bonjour': 1, 'comment': 2, 'ça': 3, 'va': 4, 'au': 5, 'revoir': 6}
input_reverse_vocab = {v: k for k, v in input_vocab.items()}
target_reverse_vocab = {v: k for k, v in target_vocab.items()}

# Convert texts to sequences
def text_to_sequence(texts, vocab):
    return [[vocab.get(word, 0) for word in text.lower().split()] for text in texts]

encoder_input_sequences = text_to_sequence(input_texts, input_vocab)
decoder_input_sequences = text_to_sequence(target_texts, target_vocab)
decoder_target_sequences = [seq[1:] + [0] for seq in decoder_input_sequences]  # shifted right for teacher forcing

# Padding sequences
def pad_sequences(sequences, max_length):
    return tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, padding='post')

max_encoder_seq_length = max(len(seq) for seq in encoder_input_sequences)
max_decoder_seq_length = max(len(seq) for seq in decoder_input_sequences)

encoder_input_sequences = pad_sequences(encoder_input_sequences, max_encoder_seq_length)
decoder_input_sequences = pad_sequences(decoder_input_sequences, max_decoder_seq_length)
decoder_target_sequences = pad_sequences(decoder_target_sequences, max_decoder_seq_length)

# Parameters
embedding_dim = 8
hidden_units = 16
input_vocab_size = len(input_vocab)
target_vocab_size = len(target_vocab)

# Encoder
encoder_inputs = Input(shape=(None,))
encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)
encoder_lstm = LSTM(hidden_units, return_sequences=False, return_state=True)
encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding)
encoder_states = [encoder_state_h, encoder_state_c]

# Decoder
decoder_inputs = Input(shape=(None,))
decoder_embedding = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)
decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
decoder_dense = Dense(target_vocab_size, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Training
decoder_target_sequences = np.expand_dims(decoder_target_sequences, -1)
model.fit([encoder_input_sequences, decoder_input_sequences], decoder_target_sequences, epochs=10, batch_size=2)

# Inference Encoder Model
encoder_model = Model(encoder_inputs, encoder_states)

# Inference Decoder Model
decoder_state_input_h = Input(shape=(hidden_units,))
decoder_state_input_c = Input(shape=(hidden_units,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
decoder_embedding_inf = Embedding(target_vocab_size, embedding_dim)
decoder_lstm_inf = LSTM(hidden_units, return_sequences=True, return_state=True)
decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm_inf(
    decoder_embedding_inf(decoder_inputs), initial_state=decoder_states_inputs
)
decoder_states = [state_h_inf, state_c_inf]
decoder_outputs_inf = decoder_dense(decoder_outputs_inf)
decoder_model = Model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs_inf] + decoder_states
)

def decode_sequence(input_seq):
    # Encode the input sequence
    states_value = encoder_model.predict(input_seq)

    # Generate empty target sequence with only the start character
    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = target_vocab['<pad>']

    # Sampling loop
    stop_condition = False
    decoded_sentence = ''
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_token = target_reverse_vocab.get(sampled_token_index, '')

        if sampled_token == '<pad>' or len(decoded_sentence) > max_decoder_seq_length:
            stop_condition = True
        else:
            decoded_sentence += ' ' + sampled_token

        # Update the target sequence
        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_token_index
        states_value = [h, c]

    return decoded_sentence.strip()

# Example translation
input_sentence = "how are you?"
input_seq = pad_sequences([text_to_sequence([input_sentence], input_vocab)[0]], max_encoder_seq_length)
translated_sentence = decode_sequence(input_seq)
print(f'Translated Sentence: {translated_sentence}')

"""# **EX-4**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding
from tensorflow.keras.optimizers import Adam

import tensorflow as tf

sentences = [
    "I love machine learning",
    "Natural language processing is fascinating"
]
pos_tags = [
    "PRP VBP NN NN",
    "NN NNS VBZ VBG"
]

# Tokenize text and tags
tokenizer_text = Tokenizer()
tokenizer_text.fit_on_texts(sentences)
text_sequences = tokenizer_text.texts_to_sequences(sentences)
vocab_size_text = len(tokenizer_text.word_index) + 1

tokenizer_tags = Tokenizer()
tokenizer_tags.fit_on_texts(pos_tags)
tag_sequences = tokenizer_tags.texts_to_sequences(pos_tags)
vocab_size_tags = len(tokenizer_tags.word_index) + 1

max_len_text = max(len(seq) for seq in text_sequences)
max_len_tags = max(len(seq) for seq in tag_sequences)
X = pad_sequences(text_sequences, maxlen=max_len_text, padding='post')
y = pad_sequences(tag_sequences, maxlen=max_len_tags, padding='post')

y = np.array([tf.keras.utils.to_categorical(seq, num_classes=vocab_size_tags) for
seq in y])

embedding_dim = 50
hidden_units = 64

encoder_inputs = Input(shape=(max_len_text,))
encoder_embedding = Embedding(input_dim=vocab_size_text,
output_dim=embedding_dim, mask_zero=True)(encoder_inputs)
encoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)
encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding)
encoder_states = [encoder_state_h, encoder_state_c]

decoder_inputs = Input(shape=(max_len_tags, vocab_size_tags))
decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)
decoder_outputs, _ , _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
decoder_dense = Dense(vocab_size_tags, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer=Adam(), loss='categorical_crossentropy',
metrics=['accuracy'])

decoder_input_data = np.zeros_like(y)
decoder_input_data[:, 1:, :] = y[:, :-1, :]

decoder_input_data[:, 0, :] = np.zeros((len(sentences), vocab_size_tags))

model.fit([X, decoder_input_data], y, epochs=10, batch_size=2, validation_split=0.1)
def predict_pos(sentence):
    seq = tokenizer_text.texts_to_sequences([sentence])
    seq = pad_sequences(seq, maxlen=max_len_text, padding='post')
    decoder_input = np.zeros((1, max_len_tags, vocab_size_tags))
    prediction = model.predict([seq, decoder_input])
    predicted_tags = np.argmax(prediction, axis=-1)
    return ' '.join(tokenizer_tags.index_word.get(tag, '') for tag in predicted_tags[0])

new_sentence = "I enjoy deep learning"
print(predict_pos(new_sentence))

"""# EXP 3"""

!pip install tensorflow numpy matplotlib

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout,BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

(x_train, _), (_, _) = mnist.load_data()
x_train = (x_train.astype(np.float32) - 127.5) / 127.5
x_train = np.expand_dims(x_train, axis=-1)

def build_generator():
    model = tf.keras.Sequential()
    model.add(Dense(256, input_dim=100))
    model.add(BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
    model.add(Dense(512))
    model.add(BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
    model.add(Dense(1024))
    model.add(BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
    model.add(Dense(28 * 28 * 1, activation='tanh'))
    model.add(Reshape((28, 28, 1)))
    return model

def build_discriminator():
    model = tf.keras.Sequential()
    model.add(Flatten(input_shape=(28, 28, 1)))
    model.add(Dense(512))
    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
    model.add(Dense(256))
    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
    model.add(Dense(1, activation='sigmoid'))
    return model

def build_gan(generator, discriminator):
    model = tf.keras.Sequential()
    model.add(generator)
    model.add(discriminator)
    return model

def compile_gan(generator, discriminator, gan):
    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
    discriminator.trainable = False
    gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))

def train_gan(generator, discriminator, gan, x_train, epochs=10000, batch_size=64):
    for epoch in range(epochs):
        # Train Discriminator
        idx = np.random.randint(0, x_train.shape[0], batch_size)
        real_imgs = x_train[idx]
        fake_imgs = generator.predict(np.random.randn(batch_size, 100))
        real_labels = np.ones((batch_size, 1))
        fake_labels = np.zeros((batch_size, 1))
        d_loss_real = discriminator.train_on_batch(real_imgs, real_labels)
        d_loss_fake = discriminator.train_on_batch(fake_imgs, fake_labels)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

def train_gan(generator, discriminator, gan, x_train, epochs=10000, batch_size=64):
    for epoch in range(epochs):
        # Train Discriminator
        idx = np.random.randint(0, x_train.shape[0], batch_size)
        real_imgs = x_train[idx]
        fake_imgs = generator.predict(np.random.randn(batch_size, 100))
        real_labels = np.ones((batch_size, 1))
        fake_labels = np.zeros((batch_size, 1))
        d_loss_real = discriminator.train_on_batch(real_imgs, real_labels)
        d_loss_fake = discriminator.train_on_batch(fake_imgs, fake_labels)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train Generator
        noise = np.random.randn(batch_size, 100) # This line was incorrectly indented. Corrected the indentation to align with the for loop.
        g_loss = gan.train_on_batch(noise, real_labels)

        # Print progress
        if epoch % 1000 == 0:
            print(f"{epoch} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]")

            # Save generated images
            save_generated_images(generator, epoch)
def save_generated_images(generator, epoch, examples=10, dim=(1, 10), figsize=(10, 1)):
    noise = np.random.randn(examples, 100)
    generated_images = generator.predict(noise)
    generated_images = (generated_images + 1) / 2.0  # Rescale images to [0, 1]

    plt.figure(figsize=figsize)
    for i in range(generated_images.shape[0]):
        plt.subplot(dim[0], dim[1], i + 1)
        plt.imshow(generated_images[i, :, :, 0], interpolation='nearest', cmap='gray')
        plt.axis('off')
    plt.tight_layout()
    plt.savefig(f'gan_generated_image_{epoch}.png')
    plt.close()

generator = build_generator()
discriminator = build_discriminator()
gan = build_gan(generator, discriminator)
compile_gan(generator, discriminator, gan)
train_gan(generator, discriminator, gan, x_train, epochs=100, batch_size=64)

"""# **EXP-8**

"""

# Install necessary libraries
!pip install tensorflow numpy matplotlib

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Load and normalize the MNIST dataset
(x_train, _), (_, _) = mnist.load_data()
x_train = (x_train.astype(np.float32) - 127.5) / 127.5  # Normalize images to [-1, 1]
x_train = np.expand_dims(x_train, axis=-1)  # Add channel dimension

def build_generator():
    model = tf.keras.Sequential()
    model.add(Dense(256, input_dim=100))
    model.add(BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
    model.add(Dense(512))
    model.add(BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
    model.add(Dense(1024))
    model.add(BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
    model.add(Dense(28 * 28 * 1, activation='tanh'))
    model.add(Reshape((28, 28, 1)))
    return model

def build_discriminator():
    model = tf.keras.Sequential()
    model.add(Flatten(input_shape=(28, 28, 1)))
    model.add(Dense(512))
    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
    model.add(Dense(256))
    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))
    model.add(Dense(1, activation='sigmoid'))
    return model

def build_gan(generator, discriminator):
    model = tf.keras.Sequential()
    model.add(generator)
    model.add(discriminator)
    return model

def compile_gan(generator, discriminator, gan):
    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
    discriminator.trainable = False
    gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))

def train_gan(generator, discriminator, gan, x_train, epochs=10000, batch_size=64):
    for epoch in range(epochs):
        # Train Discriminator
        idx = np.random.randint(0, x_train.shape[0], batch_size)
        real_imgs = x_train[idx]
        fake_imgs = generator.predict(np.random.randn(batch_size, 100))
        real_labels = np.ones((batch_size, 1))
        fake_labels = np.zeros((batch_size, 1))
        d_loss_real = discriminator.train_on_batch(real_imgs, real_labels)
        d_loss_fake = discriminator.train_on_batch(fake_imgs, fake_labels)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train Generator
        noise = np.random.randn(batch_size, 100)
        g_loss = gan.train_on_batch(noise, real_labels)

        # Print progress
        if epoch % 1000 == 0:
            print(f"{epoch} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]")

            # Save generated images
            save_generated_images(generator, epoch)

def save_generated_images(generator, epoch, examples=10, dim=(1, 10), figsize=(10, 1)):
    noise = np.random.randn(examples, 100)
    generated_images = generator.predict(noise)
    generated_images = (generated_images + 1) / 2.0  # Rescale images to [0, 1]

    plt.figure(figsize=figsize)
    for i in range(generated_images.shape[0]):
        plt.subplot(dim[0], dim[1], i + 1)
        plt.imshow(generated_images[i, :, :, 0], interpolation='nearest', cmap='gray')
        plt.axis('off')
    plt.tight_layout()
    plt.savefig(f'/content/gan_generated_image_{epoch}.png')  # Save to the Colab environment
    plt.close()

# Build and compile the GAN model
generator = build_generator()
discriminator = build_discriminator()
gan = build_gan(generator, discriminator)
compile_gan(generator, discriminator, gan)

# Start training
train_gan(generator, discriminator, gan, x_train, epochs=10000, batch_size=64)

"""## **ex-7**"""

# Step 1: Install the necessary libraries
!pip install tensorflow keras opencv-python scikit-learn

# Step 2: Import libraries
import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from sklearn.datasets import fetch_lfw_people
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from google.colab import files

# Step 3: Load dataset
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
X = lfw_people.images
y = lfw_people.target
target_names = lfw_people.target_names

# Step 4: Preprocess the data
n_samples, h, w = X.shape
X = X.reshape((n_samples, h, w, 1))  # Reshape for grayscale image input
X = X.astype('float32') / 255.0  # Normalize pixel values to [0, 1]

# Encode labels (names of people)
le = LabelEncoder()
y = le.fit_transform(y)

# Step 5: Split the dataset into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 6: Build the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(h, w, 1)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(len(target_names), activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Step 7: Train the model
model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))

# Step 8: Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'Test accuracy: {test_acc:.2f}')

# Step 9: Define a function to recognize faces in an uploaded image
def recognize_face(image_path):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale
    img = cv2.resize(img, (w, h))  # Resize to the same size as the training images
    img = img.reshape((1, h, w, 1)).astype('float32') / 255.0  # Normalize and reshape
    prediction = model.predict(img)  # Predict the label of the image
    predicted_class = np.argmax(prediction)  # Get the class with the highest probability
    return target_names[predicted_class]  # Return the predicted class label (person's name)

# Step 10: Upload a new image for testing
print("Upload an image to recognize faces:")
uploaded = files.upload()

# Get the uploaded image file path
for filename in uploaded.keys():
    image_path = filename
    # Step 11: Recognize the face in the uploaded image
    predicted_name = recognize_face(image_path)
    print(f"Predicted person: {predicted_name}")

    # Optional: Display the uploaded image
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    img = cv2.resize(img, (w, h))
    plt.imshow(img, cmap='gray')
    plt.title(f"Predicted: {predicted_name}")
    plt.show()